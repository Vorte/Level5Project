{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataIO\n",
    "from sklearn import cross_validation, linear_model\n",
    "reload(dataIO)\n",
    "import Levenshtein\n",
    "\n",
    "def error_rate(a, b):\n",
    "    msd = Levenshtein.distance(a,b)\n",
    "    \n",
    "    return 100.0*msd/max(len(a), len(b))\n",
    "\n",
    "def learn_offset(points, targets):\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(points, targets)\n",
    "    \n",
    "    return regr\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "distance expected two Strings or two Unicodes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ddb24d583b96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mtyped_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtyped_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mpred_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0merror_typed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtyped_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0merror_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4f8254b8b61d>\u001b[0m in \u001b[0;36merror_rate\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0merror_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmsd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLevenshtein\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m100.0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmsd\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: distance expected two Strings or two Unicodes"
     ]
    }
   ],
   "source": [
    "### POOLED\n",
    "\n",
    "import numpy as np\n",
    "import ast\n",
    "import dataIO\n",
    "reload(dataIO)\n",
    "\n",
    "userId = 11\n",
    "locations, bod, targets_x, targets_y, y, touch_centers = dataIO.process_twohand(userId)\n",
    "\n",
    "locations = np.array(locations)\n",
    "targets_x = np.array(targets_x)\n",
    "targets_y = np.array(targets_y)\n",
    "\n",
    "regr_x = learn_offset(locations, targets_x)\n",
    "regr_y = learn_offset(locations, targets_y)\n",
    "\n",
    "with open(\"/home/dimitar/Desktop/Python/Level5Project/Loggingapp/dataset.txt\") as f:\n",
    "    pool = f.read().splitlines()\n",
    "\n",
    "#def read(userId):\n",
    "req_sentences = []\n",
    "typed_sentences = []\n",
    "pred_sentences = []\n",
    "#for i in range(13, 25):\n",
    "for i in [15, 17, 21]:\n",
    "    column_index = 0\n",
    "    if i in [15, 17, 21]:\n",
    "        column_index = 1\n",
    "\n",
    "    with open(\"/home/dimitar/Desktop/Python/experiment/results/\"\n",
    "               +str(userId)+\"_\"+str(i)+\"up.txt\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        touches = map(lambda x: x.split('\\t'), lines[1:])\n",
    "\n",
    "    req_string = ''.join(np.array(touches)[:,column_index]) \n",
    "    locations = map(lambda x: list(ast.literal_eval(x)),\n",
    "                            np.array(touches)[:,(column_index+2)])\n",
    "   \n",
    "    for sentence in pool: \n",
    "        index = req_string.find(sentence)\n",
    "        if index!=-1:\n",
    "            req_sentences.append(sentence)\n",
    "            typed_locations = np.array(locations[index:index+len(sentence)])\n",
    "            \n",
    "            pred_x = regr_x.predict(typed_locations)\n",
    "            pred_y = regr_y.predict(typed_locations)\n",
    "            pred = typed_locations + np.dstack((pred_x, pred_y))[0]\n",
    "            \n",
    "            typed_sentences.append(dataIO.typed_string(typed_locations))\n",
    "            pred_sentences.append(dataIO.typed_string(pred))\n",
    "\n",
    "error_typed = []\n",
    "error_pred = []\n",
    "for i in range(len(req_sentences)):    \n",
    "    req_sentence = req_sentences[i]\n",
    "    typed_sentence = typed_sentences[i]\n",
    "    pred_sentence = pred_sentences[i]\n",
    "    error_typed.append(error_rate(req_sentence, typed_sentence))\n",
    "    error_pred.append(error_rate(req_sentence, pred_sentence))\n",
    "\n",
    "print\n",
    "print (\"Mean error rate before: %.2f %%\"% np.mean(np.array(error_typed)))\n",
    "print (\"Mean error rate after: %.2f %%\"% np.mean(np.array(error_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'float64' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4c47800d1424>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_typed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-4c47800d1424>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(userId)\u001b[0m\n\u001b[0;32m     98\u001b[0m                                 np.array(touches)[:,(column_index+2)]))\n\u001b[0;32m     99\u001b[0m         bod = np.array(map(lambda x: ast.literal_eval(x),\n\u001b[1;32m--> 100\u001b[1;33m                                 np.array(touches, dtype=float64)[:,(column_index+4)]))\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'float64' is not defined"
     ]
    }
   ],
   "source": [
    "#### PREDICT POSTURES\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import Levenshtein, ast, dataIO\n",
    "from sklearn import cross_validation, linear_model, preprocessing, svm\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "reload(dataIO)\n",
    "\n",
    "def error_rate(a, b):\n",
    "    msd = Levenshtein.distance(a,b)\n",
    "    \n",
    "    return 100.0*msd/max(len(a), len(b))\n",
    "\n",
    "def learn_offset(points, targets):\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(points, targets)\n",
    "    \n",
    "    return regr\n",
    "\n",
    "\n",
    "postures = {\"left_hand\":[\"4\", \"8\", \"11\"], \"right_hand\":[\"1\", \"7\", \"10\"], \n",
    "            \"index_finger\":[\"3\", \"5\", \"12\"], \"two_hand\":[\"2\", \"6\", \"9\"]}\n",
    "                \n",
    "userId = 11\n",
    "\n",
    "def run(userId):\n",
    "    keys = postures.keys()\n",
    "    locations = []\n",
    "    bod = []\n",
    "    targets_x = []\n",
    "    targets_y = []\n",
    "    y = []\n",
    "    touch_centers = []\n",
    "\n",
    "    posture = 0\n",
    "    for key in keys:\n",
    "        filenos = postures[key]\n",
    "        if key == \"two_hand\":\n",
    "            a, b, c, d, e, f = dataIO.process_twohand(userId, posture)\n",
    "            posture += 2\n",
    "        else:\n",
    "            a, b, c, d, e, f = dataIO.process_posture(userId, filenos, posture)\n",
    "            posture += 1\n",
    "\n",
    "        locations += a\n",
    "        bod += b\n",
    "        targets_x += c \n",
    "        targets_y += d \n",
    "        y += e\n",
    "        touch_centers += f\n",
    "\n",
    "    locations = np.array(locations)\n",
    "    bod = np.array(bod)\n",
    "    targets_x = np.array(targets_x)\n",
    "    targets_y = np.array(targets_y)\n",
    "    y = np.array(y)\n",
    "    touch_centers = np.array(touch_centers)\n",
    "\n",
    "    scaler = preprocessing.StandardScaler().fit(bod)  \n",
    "    bod_scaled = scaler.transform(bod)\n",
    "\n",
    "    tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1, 0.1 ,1e-2, 1e-3, 1e-4],\n",
    "                         'C': [0.001, 0.01, 0.1 ,1, 10, 100, 1000]},\n",
    "                        {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1 ,1, 10, 100, 1000]}]\n",
    "\n",
    "    #clf = GridSearchCV(svm.SVC(C=1, cache_size=500), tuned_parameters)\n",
    "    clf = svm.SVC(C=100, kernel='rbf', gamma=0.01, cache_size=500)\n",
    "    clf.fit(bod_scaled, y)\n",
    "\n",
    "    regr_x = [] \n",
    "    regr_y = []\n",
    "\n",
    "    for i in range(0,5):\n",
    "        index = np.where(y==i)[0]\n",
    "        regr_x.append(learn_offset(locations[index], targets_x[index]))\n",
    "        regr_y.append(learn_offset(locations[index], targets_y[index]))\n",
    "\n",
    "    with open(\"/home/dimitar/Desktop/Python/Level5Project/Loggingapp/dataset.txt\") as f:\n",
    "        pool = f.read().splitlines()\n",
    "\n",
    "    req_sentences = []\n",
    "    typed_sentences = []\n",
    "    pred_sentences = []\n",
    "\n",
    "    for i in range(13, 25):\n",
    "        column_index = 0\n",
    "        if i in [15, 17, 21]:\n",
    "            column_index = 1\n",
    "\n",
    "        with open(\"/home/dimitar/Desktop/Python/experiment/results/\"\n",
    "                   +str(userId)+\"_\"+str(i)+\"up.txt\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "            touches = map(lambda x: x.split('\\t'), lines[1:])\n",
    "\n",
    "        req_string = ''.join(np.array(touches)[:,column_index]) \n",
    "        locations = np.array(map(lambda x: ast.literal_eval(x),\n",
    "                                np.array(touches)[:,(column_index+2)]))\n",
    "        bod = np.array(map(lambda x: ast.literal_eval(x),\n",
    "                                np.array(touches)[:,(column_index+4)]))\n",
    "\n",
    "        for sentence in pool: \n",
    "            index = req_string.find(sentence)\n",
    "            if index!=-1:\n",
    "                req_sentences.append(sentence)\n",
    "                typed_locations = np.array(locations[index:index+len(sentence)])\n",
    "                typed_bod = np.array(bod[index:index+len(sentence)])\n",
    "\n",
    "                bod_data = scaler.transform(typed_bod)\n",
    "                pred = clf.predict(bod_data)\n",
    "\n",
    "                pred_x = []\n",
    "                pred_y = []\n",
    "                for i in range(len(sentence)):\n",
    "                    regr_no = pred[i]\n",
    "                    pred_x.append(regr_x[regr_no].predict(typed_locations[i]))\n",
    "                    pred_y.append(regr_y[regr_no].predict(typed_locations[i]))\n",
    "\n",
    "                new_points = typed_locations + np.dstack((pred_x, pred_y))[0]\n",
    "                typed_sentences.append(dataIO.typed_string(typed_locations))\n",
    "                pred_sentences.append(dataIO.typed_string(new_points))\n",
    "\n",
    "\n",
    "    error_typed = []\n",
    "    error_pred = []\n",
    "    for i in range(len(req_sentences)):    \n",
    "        req_sentence = req_sentences[i]\n",
    "        typed_sentence = typed_sentences[i]\n",
    "        pred_sentence = pred_sentences[i]\n",
    "        error_typed.append(error_rate(req_sentence, typed_sentence))\n",
    "        error_pred.append(error_rate(req_sentence, pred_sentence))\n",
    "\n",
    "    print\n",
    "    print (\"Mean error rate before: %.2f %%\"% np.mean(np.array(error_typed)))\n",
    "    print (\"Mean error rate after: %.2f %%\"% np.mean(np.array(error_pred)))\n",
    "    \n",
    "    return np.mean(np.array(error_typed)), np.mean(np.array(error_pred))\n",
    "    \n",
    "run(3)\n",
    "    \n",
    "    \n",
    "error_diff = []\n",
    "for i in range(3, 18):\n",
    "    typed, pred = run(i)\n",
    "    error_diff.append(pred-typed)\n",
    "    \n",
    "print error_diff    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
